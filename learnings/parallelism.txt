
https://medium.com/@rico098098/load-testing-with-python-fea13369af43
https://dev.to/coderatul/threading-vs-asyncio-vs-multiprocessing-10ed?utm_source=chatgpt.com
https://cgnarendiran.github.io/blog/asyncio-vs-multithreading-vs-multiprocessing/



syncgronous execution- you are awaitign and blocked even thoguht you are not doing any work 


Multihreading vs async 

If you are executing the app on a single hread, you are implementing the code sequentially. youa re waitting for those io operations syncronously. 

so lets spin some other thread to do some other work but since shared emmeory, yiu run into race issues and deadlokcing 

So what u do is intiialise a single thread and when it sends some io request, and it tells the io driver to call it back once the repsonse is ready to be recieved
What this does is prevent the thread from waiting and do the operation in async. This gives it time to execute some other 
code. - non-blocking asnc thread
At the same time there are tasks which are actually blockign the event loop for eg cpu operations or io drivers that are not built for async operations

act asycnhornously but look synchronous to you!!! 



Multiprocessing 
spinning up threads in a process, spinning up unique processes with their own memory and do interprocess communication using sockets or redis db

They can, but only if you share mutable state across threads and coordinate it poorly. If you don’t share (or only pass data in/out), you won’t have races or deadlocks.


Think of each request as its own sequential pipeline (A → B → C). With async, the single event loop 
interleaves many pipelines at their await points, so while one request is waiting on I/O, the loop runs another request’s step.

in a web API framework like FastAPI/Starlette, the ASGI server (uvicorn, hypercorn, etc.) is doing the “asyncio.gather dance” for you under the hood.

the key idea: await propagates up the call chain.


flask (0default mode):
each worker handles one request at a time 

FastAPI
One worker can handle many requests at once as long as your route function is async def and uses await for I/O.

The await keyword suspends the execution of the surrounding coroutine and passes control back to the event loop
.gather  is used to run multiple courittine obejcts at the same time 
The async def syntax construct introduces either a coroutine function or an asynchronous generator.

A coroutine is an object that can suspend its execution and resume it later. In the meantime, it
can pass the control to an event loop, which can execute another coroutine. 

await pauses your function until the awaited operation finishes.
While paused, the event loop can run other tasks (other requests or coroutines).
Inside a single request, operations still run sequentially unless you start them together (e.g., with asyncio.gather()).

asyncio.sleep is the non blcokign io version of time.sleep

Uvicorn has one main thread 
coroutines run in the even loop in the main thread 

all async fucntions use the single event loop 
for any normal functions, a seprate thread is created for that endpoint

async endpoint
sync endpoint

A good practise is to have enviroment variables in a .env file

with anyio, we can easily call an async fucntion from a sync functioon and vice versa and create a new thread for it 
(so that it does not block in the latter case and we does not harm the current event loop since the even loop async.run is only called once)


•	Raw threading.Thread is unmanaged; loop won’t await it.
•	Use await anyio.to_thread.run_sync(...) (or process pools for heavy CPU) to integrate with async.

1.	Workers = separate processes, each with their own event loop (async) or threadpool (sync).
2.	Sync endpoints (no async/await) → concurrency limited by per-worker threadpool size; more workers = more total concurrent requests.
3.	Async endpoints + non-blocking I/O → one worker can juggle many concurrent requests; more workers only help with CPU/pool caps or long-lived connections.